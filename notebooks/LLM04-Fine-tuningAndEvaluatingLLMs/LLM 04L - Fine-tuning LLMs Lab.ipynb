{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-sandbox\n",
        "# MAGIC\n",
        "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
        "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MAGIC\n",
        "# 04L - Fine-tuning LLMs\n",
        "In this lab, we will apply the fine-tuning learnings from the demo Notebook. The aim of this lab is to fine-tune an instruction-following LLM.\n",
        "# MAGIC\n",
        "### ![Dolly](https://files.training.databricks.com/images/llm/dolly_small.png) Learning Objectives\n",
        "1. Prepare a novel dataset\n",
        "1. Fine-tune the T5-small model to classify movie reviews.\n",
        "1. Leverage DeepSpeed to enhance training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "assert \"gpu\" in spark.conf.get(\"spark.databricks.clusterUsageTags.sparkVersion\"), \"THIS LAB REQUIRES THAT A GPU MACHINE AND RUNTIME IS UTILIZED.\""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MAGIC\n",
        "## Classroom Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MAGIC %pip install rouge_score==0.1.2"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MAGIC %run ../Includes/Classroom-Setup"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"Username:          {DA.username}\")\n",
        "print(f\"Working Directory: {DA.paths.working_dir}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MAGIC %load_ext autoreload\n",
        "# MAGIC %autoreload 2"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating a local temporary directory on the Driver. This will serve as a root directory for the intermediate model checkpoints created during the training process. The final model will be persisted to DBFS."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import tempfile\n",
        "\n",
        "tmpdir = tempfile.TemporaryDirectory()\n",
        "local_training_root = tmpdir.name"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fine-Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    TrainingArguments,\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        "    Trainer,\n",
        "    AutoModelForCausalLM,\n",
        "    DataCollatorForLanguageModeling,\n",
        ")\n",
        "\n",
        "import evaluate\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 1: Data Preparation\n",
        "For the instruction-following use cases we need a dataset that consists of prompt/response pairs along with any contextual information that can be used as input when training the model. The [databricks/databricks-dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k) is one such dataset that provides high-quality, human-generated prompt/response pairs. \n",
        "# MAGIC\n",
        "Let's start by loading this dataset using the `load_dataset` functionality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO\n",
        "ds = <FILL_IN>"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test your answer. DO NOT MODIFY THIS CELL.\n",
        "\n",
        "dbTestQuestion4_1(ds)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 2: Select pre-trained model\n",
        "# MAGIC\n",
        "The model that we are going to fine-tune is [pythia-70m-deduped](https://huggingface.co/EleutherAI/pythia-70m-deduped). This model is one of a Pythia Suite of models that have been developed to support interpretability research.\n",
        "# MAGIC\n",
        "Let's define the pre-trained model checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO\n",
        "model_checkpoint = <FILL_IN>"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test your answer. DO NOT MODIFY THIS CELL.\n",
        "\n",
        "dbTestQuestion4_2(model_checkpoint)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 3: Load and Configure\n",
        "# MAGIC\n",
        "The next task is to load and configure the tokenizer for this model. The instruction-following process builds a body of text that contains the instruction, context input, and response values from the dataset. The body of text also includes some special tokens to identify the sections of the text. These tokens are generally configurable, and need to be added to the tokenizer.\n",
        "# MAGIC\n",
        "Let's go ahead and load the tokenizer for the pre-trained model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO\n",
        "# load the tokenizer that was used for the model\n",
        "tokenizer = <FILL_IN>\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.add_special_tokens(\n",
        "    {\"additional_special_tokens\": [\"### End\", \"### Instruction:\", \"### Response:\\n\"]}\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test your answer. DO NOT MODIFY THIS CELL.\n",
        "\n",
        "dbTestQuestion4_3(tokenizer)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ### Question 4: Tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `tokenize` method below builds the body of text for each prompt/response."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "remove_columns = [\"instruction\", \"response\", \"context\", \"category\"]\n",
        "\n",
        "\n",
        "def tokenize(x: dict, max_length: int = 1024) -> dict:\n",
        "    \"\"\"\n",
        "    For a dictionary example of instruction, response, and context a dictionary of input_id and attention mask is returned\n",
        "    \"\"\"\n",
        "    instr = x[\"instruction\"]\n",
        "    resp = x[\"response\"]\n",
        "    context = x[\"context\"]\n",
        "\n",
        "    instr_part = f\"### Instruction:\\n{instr}\"\n",
        "    context_part = \"\"\n",
        "    if context:\n",
        "        context_part = f\"\\nInput:\\n{context}\\n\"\n",
        "    resp_part = f\"### Response:\\n{resp}\"\n",
        "\n",
        "    text = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "{instr_part}\n",
        "{context_part}\n",
        "{resp_part}\n",
        "\n",
        "### End\n",
        "\"\"\"\n",
        "    return tokenizer(text, max_length=max_length, truncation=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's `tokenize` the Dolly training dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO\n",
        "tokenized_dataset = <FILL_IN>"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test your answer. DO NOT MODIFY THIS CELL.\n",
        "\n",
        "dbTestQuestion4_4(tokenized_dataset)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 5: Setup Training\n",
        "# MAGIC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To setup the fine-tuning process we need to define the `TrainingArguments`.\n",
        "# MAGIC\n",
        "Let's configure the training to have **10** training epochs (`num_train_epochs`) with a per device batch size of **8**. The optimizer (`optim`) to be used should be `adamw_torch`. Finally, the reporting (`report_to`) list should be set to *tensorboard*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO\n",
        "checkpoint_name = \"test-trainer-lab\"\n",
        "local_checkpoint_path = os.path.join(local_training_root, checkpoint_name)\n",
        "training_args = <FILL_IN>"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "checkpoint_name = \"test-trainer-lab\""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test your answer. DO NOT MODIFY THIS CELL.\n",
        "\n",
        "dbTestQuestion4_5(training_args)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ### Question 6: AutoModelForCausalLM\n",
        "# MAGIC\n",
        "The pre-trained `pythia-70m-deduped` model can be loaded using the [AutoModelForCausalLM](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM) class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO\n",
        "# load the pre-trained model\n",
        "model = <FILL_IN>"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test your answer. DO NOT MODIFY THIS CELL.\n",
        "\n",
        "dbTestQuestion4_6(model)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 7: Initialize the Trainer\n",
        "# MAGIC\n",
        "Unlike the IMDB dataset used in the earlier Notebook, the Dolly dataset only contains a single *train* dataset. Let's go ahead and create a [`train_test_split`](https://huggingface.co/docs/datasets/v2.12.0/en/package_reference/main_classes#datasets.Dataset.train_test_split) of the train dataset.\n",
        "# MAGIC\n",
        "Also, let's initialize the [`Trainer`](https://huggingface.co/docs/transformers/main_classes/trainer) with model, training arguments, the train & test datasets, tokenizer, and data collator. Here we will use the [`DataCollatorForLanguageModeling`](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorForLanguageModeling)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO\n",
        "# used to assist the trainer in batching the data\n",
        "TRAINING_SIZE=6000\n",
        "SEED=42\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=False, return_tensors=\"pt\", pad_to_multiple_of=8\n",
        ")\n",
        "split_dataset = <FILL_IN>\n",
        "trainer = <FILL_IN>"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test your answer. DO NOT MODIFY THIS CELL.\n",
        "\n",
        "dbTestQuestion4_7(trainer)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 8: Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MAGIC\n",
        "Before starting the training process, let's turn on Tensorboard. This will allow us to monitor the training process as checkpoint logs are created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "tensorboard_display_dir = f\"{local_checkpoint_path}/runs\""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MAGIC %load_ext tensorboard\n",
        "# MAGIC %tensorboard --logdir '{tensorboard_display_dir}'"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Start the fine-tuning process!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO\n",
        "# invoke training - note this will take approx. 30min\n",
        "<FILL_IN>\n",
        "\n",
        "# save model to the local checkpoint\n",
        "trainer.save_model()\n",
        "trainer.save_state()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test your answer. DO NOT MODIFY THIS CELL.\n",
        "\n",
        "dbTestQuestion4_8(trainer)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# persist the fine-tuned model to DBFS\n",
        "final_model_path = f\"{DA.paths.working_dir}/llm04_fine_tuning/{checkpoint_name}\"\n",
        "trainer.save_model(output_dir=final_model_path)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fine_tuned_model = AutoModelForCausalLM.from_pretrained(final_model_path)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Recall that the model was trained using a body of text that contained an instruction and its response. A similar body of text, or prompt, needs to be provided when testing the model. The prompt that is provided only contains an instruction though. The model will `generate` the response accordingly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def to_prompt(instr: str, max_length: int = 1024) -> dict:\n",
        "    text = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{instr}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "    return tokenizer(text, return_tensors=\"pt\", max_length=max_length, truncation=True)\n",
        "\n",
        "\n",
        "def to_response(prediction):\n",
        "    decoded = tokenizer.decode(prediction)\n",
        "    # extract the Response from the decoded sequence\n",
        "    m = re.search(r\"#+\\s*Response:\\s*(.+?)#+\\s*End\", decoded, flags=re.DOTALL)\n",
        "    res = \"Failed to find response\"\n",
        "    if m:\n",
        "        res = m.group(1).strip()\n",
        "    else:\n",
        "        m = re.search(r\"#+\\s*Response:\\s*(.+)\", decoded, flags=re.DOTALL)\n",
        "        if m:\n",
        "            res = m.group(1).strip()\n",
        "    return res"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import re\n",
        "# NOTE: this cell can take up to 5mins\n",
        "res = []\n",
        "for i in range(100):\n",
        "    instr = ds[\"train\"][i][\"instruction\"]\n",
        "    resp = ds[\"train\"][i][\"response\"]\n",
        "    inputs = to_prompt(instr)\n",
        "    pred = fine_tuned_model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        max_new_tokens=128,\n",
        "    )\n",
        "    res.append((instr, resp, to_response(pred[0])))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pdf = pd.DataFrame(res, columns=[\"instruction\", \"response\", \"generated\"])\n",
        "display(pdf)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**CONGRATULATIONS**\n",
        "# MAGIC\n",
        "You have just taken the first step toward fine-tuning your own slimmed down version of [Dolly](https://github.com/databrickslabs/dolly)! \n",
        "# MAGIC\n",
        "Unfortunately, it does not seem to be too generative at the moment. Perhaps, with some additional training and data the model could be more capable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 9: Evaluation\n",
        "# MAGIC\n",
        "Although the current model is under-trained, it is worth evaluating the responses to get a general sense of how far off the model is at this point.\n",
        "# MAGIC\n",
        "Let's compute the ROGUE metrics between the reference response and the generated responses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "nltk.download(\"punkt\")\n",
        "\n",
        "rouge_score = evaluate.load(\"rouge\")\n",
        "\n",
        "\n",
        "def compute_rouge_score(generated, reference):\n",
        "    \"\"\"\n",
        "    Compute ROUGE scores on a batch of articles.\n",
        "\n",
        "    This is a convenience function wrapping Hugging Face `rouge_score`,\n",
        "    which expects sentences to be separated by newlines.\n",
        "\n",
        "    :param generated: Summaries (list of strings) produced by the model\n",
        "    :param reference: Ground-truth summaries (list of strings) for comparison\n",
        "    \"\"\"\n",
        "    generated_with_newlines = [\"\\n\".join(sent_tokenize(s.strip())) for s in generated]\n",
        "    reference_with_newlines = [\"\\n\".join(sent_tokenize(s.strip())) for s in reference]\n",
        "    return rouge_score.compute(\n",
        "        predictions=generated_with_newlines,\n",
        "        references=reference_with_newlines,\n",
        "        use_stemmer=True,\n",
        "    )"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO\n",
        "rouge_scores = <FILL_IN>\n",
        "display(<FILL_IN>)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test your answer. DO NOT MODIFY THIS CELL.\n",
        "\n",
        "dbTestQuestion4_9(rouge_scores)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MAGIC\n",
        "## Clean up Classroom\n",
        "# MAGIC\n",
        "Run the following cell to remove lessons-specific assets created during this lesson."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "tmpdir.cleanup()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ## Submit your Results (edX Verified Only)\n",
        "# MAGIC\n",
        "To get credit for this lab, click the submit button in the top right to report the results. If you run into any issues, click `Run` -> `Clear state and run all`, and make sure all tests have passed before re-submitting. If you accidentally deleted any tests, take a look at the notebook's version history to recover them or reload the notebooks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-sandbox\n",
        "&copy; 2023 Databricks, Inc. All rights reserved.<br/>\n",
        "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
        "<br/>\n",
        "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}