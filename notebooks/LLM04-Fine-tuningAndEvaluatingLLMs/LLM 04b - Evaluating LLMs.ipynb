{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Large Language Models (LLMs)\n",
    "This notebook demonstrates methods for evaluating LLMs.  We focus on the task of summarization and cover accuracy, ROUGE-N, and perplexity.\n",
    "\n",
    "### ![Dolly](https://files.training.databricks.com/images/llm/dolly_small.png) Learning Objectives\n",
    "1. Know how to compute ROUGE-N and other metrics.\n",
    "2. Gain an intuitive understanding of ROUGE-N.\n",
    "3. Test various models and model sizes on the same data, and compare their results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classroom Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge_score==0.1.2\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting absl-py (from rouge_score==0.1.2)\n",
      "  Obtaining dependency information for absl-py from https://files.pythonhosted.org/packages/01/e4/dc0a1dcc4e74e08d7abedab278c795eef54a224363bb18f5692f416d834f/absl_py-2.0.0-py3-none-any.whl.metadata\n",
      "  Downloading absl_py-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: nltk in /Users/vmishra/miniconda3/envs/llm/lib/python3.10/site-packages (from rouge_score==0.1.2) (3.8.1)\n",
      "Requirement already satisfied: numpy in /Users/vmishra/miniconda3/envs/llm/lib/python3.10/site-packages (from rouge_score==0.1.2) (1.24.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /Users/vmishra/miniconda3/envs/llm/lib/python3.10/site-packages (from rouge_score==0.1.2) (1.16.0)\n",
      "Requirement already satisfied: click in /Users/vmishra/miniconda3/envs/llm/lib/python3.10/site-packages (from nltk->rouge_score==0.1.2) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/vmishra/miniconda3/envs/llm/lib/python3.10/site-packages (from nltk->rouge_score==0.1.2) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/vmishra/miniconda3/envs/llm/lib/python3.10/site-packages (from nltk->rouge_score==0.1.2) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /Users/vmishra/miniconda3/envs/llm/lib/python3.10/site-packages (from nltk->rouge_score==0.1.2) (4.66.1)\n",
      "Downloading absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m688.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m790.4 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24932 sha256=3ec246558de2a34b42335c06c6bb03e8cd4d6bce76a486c38929c8056f15fd11\n",
      "  Stored in directory: /Users/vmishra/Library/Caches/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: absl-py, rouge_score\n",
      "Successfully installed absl-py-2.0.0 rouge_score-0.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rouge_score==0.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAGIC %run ../Includes/Classroom-Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## How can we evaluate summarization?\n",
    "Suppose you are developing a smartphone news app and need to display automatically generated summaries of breaking news articles.  How can you evaluate whether or not the summaries you are generating are good?\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1V6cMD1LgivCb850JDhva1DO9EWVH8rJ7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We will use a subset of the `cnn_dailymail` dataset from See et al., 2017, downloadable from the [Hugging Face `datasets` hub](https://huggingface.co/datasets/cnn_dailymail).\n",
    "\n",
    "This dataset provides news article paired with summaries (in the \"highlights\" column).  Let's load the data and take a look at some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving notices: ...working... done\n",
      "Collecting package metadata (current_repodata.json): / WARNING conda.models.version:get_matcher(548): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.7.1.*, but conda is ignoring the .* and treating it as 1.7.1\n",
      "done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.5.2\n",
      "  latest version: 23.9.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=23.9.0\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/vmishra/miniconda3/envs/llm\n",
      "\n",
      "  added / updated specs:\n",
      "    - datasets\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    datasets-2.14.5            |             py_0         354 KB  huggingface\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         354 KB\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  datasets           conda-forge::datasets-2.14.5-pyhd8ed1~ --> huggingface::datasets-2.14.5-py_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "                                                                                \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c huggingface -c conda-forge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /Users/vmishra/miniconda3/envs/llm/lib/python3.10/site-packages (2023.1.0)\n",
      "Collecting fsspec[http]>=2021.05.0\n",
      "  Obtaining dependency information for fsspec[http]>=2021.05.0 from https://files.pythonhosted.org/packages/fe/d3/e1aa96437d944fbb9cc95d0316e25583886e9cd9e6adc07baad943524eda/fsspec-2023.9.2-py3-none-any.whl.metadata\n",
      "  Using cached fsspec-2023.9.2-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: requests in /Users/vmishra/miniconda3/envs/llm/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0) (2.31.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/vmishra/miniconda3/envs/llm/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0) (3.8.5)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/vmishra/miniconda3/envs/llm/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/vmishra/miniconda3/envs/llm/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/vmishra/miniconda3/envs/llm/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/vmishra/miniconda3/envs/llm/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/vmishra/miniconda3/envs/llm/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/vmishra/miniconda3/envs/llm/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/vmishra/miniconda3/envs/llm/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vmishra/miniconda3/envs/llm/lib/python3.10/site-packages (from requests->fsspec[http]>=2021.05.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/vmishra/miniconda3/envs/llm/lib/python3.10/site-packages (from requests->fsspec[http]>=2021.05.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vmishra/miniconda3/envs/llm/lib/python3.10/site-packages (from requests->fsspec[http]>=2021.05.0) (2023.7.22)\n",
      "Using cached fsspec-2023.9.2-py3-none-any.whl (173 kB)\n",
      "Installing collected packages: fsspec\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.1.0\n",
      "    Uninstalling fsspec-2023.1.0:\n",
      "      Successfully uninstalled fsspec-2023.1.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 2.14.5 requires fsspec[http]<2023.9.0,>=2023.1.0, but you have fsspec 2023.9.2 which is incompatible.\n",
      "s3fs 2023.1.0 requires fsspec==2023.1.0, but you have fsspec 2023.9.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed fsspec-2023.9.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U \"fsspec[http]>=2021.05.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'datasets' has no attribute 'utils' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      4\u001b[0m full_dataset \u001b[38;5;241m=\u001b[39m load_dataset(\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcnn_dailymail\u001b[39m\u001b[38;5;124m\"\u001b[39m, version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m, cache_dir\u001b[38;5;241m=\u001b[39mDA\u001b[38;5;241m.\u001b[39mpaths\u001b[38;5;241m.\u001b[39mdatasets\n\u001b[1;32m      6\u001b[0m )  \u001b[38;5;66;03m# Note: We specify cache_dir to use pre-cached data.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Use a small sample of the data during this lab, for speed.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/datasets/__init__.py:31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfingerprint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m disable_caching, enable_caching, is_caching_enabled, set_caching_enabled\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minfo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetInfo, MetricInfo\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minspect\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     32\u001b[0m     get_dataset_config_info,\n\u001b[1;32m     33\u001b[0m     get_dataset_config_names,\n\u001b[1;32m     34\u001b[0m     get_dataset_infos,\n\u001b[1;32m     35\u001b[0m     get_dataset_split_names,\n\u001b[1;32m     36\u001b[0m     inspect_dataset,\n\u001b[1;32m     37\u001b[0m     inspect_metric,\n\u001b[1;32m     38\u001b[0m     list_datasets,\n\u001b[1;32m     39\u001b[0m     list_metrics,\n\u001b[1;32m     40\u001b[0m )\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miterable_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IterableDataset\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset, load_dataset_builder, load_from_disk, load_metric\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/datasets/inspect.py:31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownload\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstreaming_download_manager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StreamingDownloadManager\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minfo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetInfo\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     32\u001b[0m     dataset_module_factory,\n\u001b[1;32m     33\u001b[0m     get_dataset_builder_class,\n\u001b[1;32m     34\u001b[0m     import_main_class,\n\u001b[1;32m     35\u001b[0m     load_dataset_builder,\n\u001b[1;32m     36\u001b[0m     metric_module_factory,\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecation_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecated\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfile_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m relative_to_absolute_path\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/datasets/load.py:58\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Metric\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnaming\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m camelcase_to_snakecase, snakecase_to_camelcase\n\u001b[0;32m---> 58\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpackaged_modules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     59\u001b[0m     _EXTENSION_TO_MODULE,\n\u001b[1;32m     60\u001b[0m     _MODULE_SUPPORTS_METADATA,\n\u001b[1;32m     61\u001b[0m     _MODULE_TO_EXTENSIONS,\n\u001b[1;32m     62\u001b[0m     _PACKAGED_DATASETS_MODULES,\n\u001b[1;32m     63\u001b[0m     _hash_python_lines,\n\u001b[1;32m     64\u001b[0m )\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msplits\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Split\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecation_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecated\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/datasets/packaged_modules/__init__.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhashlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sha256\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, List\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m arrow\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudiofolder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audiofolder\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcsv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m csv\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/datasets/packaged_modules/arrow/arrow.py:11\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtable\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m table_cast\n\u001b[0;32m---> 11\u001b[0m logger \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241m.\u001b[39mlogging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mArrowConfig\u001b[39;00m(datasets\u001b[38;5;241m.\u001b[39mBuilderConfig):\n\u001b[1;32m     16\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"BuilderConfig for Arrow.\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'datasets' has no attribute 'utils' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "full_dataset = load_dataset(\n",
    "    \"cnn_dailymail\", version=\"3.0.0\", cache_dir=DA.paths.datasets\n",
    ")  # Note: We specify cache_dir to use pre-cached data.\n",
    "\n",
    "# Use a small sample of the data during this lab, for speed.\n",
    "sample_size = 100\n",
    "sample = (\n",
    "    full_dataset[\"train\"]\n",
    "    .filter(lambda r: \"CNN\" in r[\"article\"][:25])\n",
    "    .shuffle(seed=42)\n",
    "    .select(range(sample_size))\n",
    ")\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(sample.to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_article = sample[\"article\"][0]\n",
    "example_summary = sample[\"highlights\"][0]\n",
    "print(f\"Article:\\n{example_article}\\n\")\n",
    "print(f\"Summary:\\n{example_summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import gc\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(data: list, batch_size: int):\n",
    "    \"\"\"\n",
    "    Creates batches of size `batch_size` from a list.\n",
    "    \"\"\"\n",
    "    s = 0\n",
    "    e = s + batch_size\n",
    "    while s < len(data):\n",
    "        yield data[s:e]\n",
    "        s = e\n",
    "        e = min(s + batch_size, len(data))\n",
    "\n",
    "\n",
    "def summarize_with_t5(\n",
    "    model_checkpoint: str, articles: list, batch_size: int = 8\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Compute summaries using a T5 model.\n",
    "    This is similar to a `pipeline` for a T5 model but does tokenization manually.\n",
    "\n",
    "    :param model_checkpoint: Name for a model checkpoint in Hugging Face, such as \"t5-small\" or \"t5-base\"\n",
    "    :param articles: List of strings, where each string represents one article.\n",
    "    :return: List of strings, where each string represents one article's generated summary\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\n",
    "        model_checkpoint, cache_dir=DA.paths.datasets\n",
    "    ).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_checkpoint, model_max_length=1024, cache_dir=DA.paths.datasets\n",
    "    )\n",
    "\n",
    "    def perform_inference(batch: list) -> list:\n",
    "        inputs = tokenizer(\n",
    "            batch, max_length=1024, return_tensors=\"pt\", padding=True, truncation=True\n",
    "        )\n",
    "\n",
    "        summary_ids = model.generate(\n",
    "            inputs.input_ids.to(device),\n",
    "            attention_mask=inputs.attention_mask.to(device),\n",
    "            num_beams=2,\n",
    "            min_length=0,\n",
    "            max_length=40,\n",
    "        )\n",
    "        return tokenizer.batch_decode(summary_ids, skip_special_tokens=True)\n",
    "\n",
    "    res = []\n",
    "\n",
    "    summary_articles = list(map(lambda article: \"summarize: \" + article, articles))\n",
    "    for batch in batch_generator(summary_articles, batch_size=batch_size):\n",
    "        res += perform_inference(batch)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # clean up\n",
    "    del tokenizer\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_small_summaries = summarize_with_t5(\"t5-small\", sample[\"article\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_summaries = sample[\"highlights\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    pd.DataFrame.from_dict(\n",
    "        {\n",
    "            \"generated\": t5_small_summaries,\n",
    "            \"reference\": reference_summaries,\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may see some warning messages in the output above.  While pipelines are handy, they provide less control over the tokenizer and model; we will dive deeper later.\n",
    "\n",
    "But first, let's see how our summarization pipeline does!  We'll compute 0/1 accuracy, a classic ML evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 0.0\n",
    "for i in range(len(reference_summaries)):\n",
    "    generated_summary = t5_small_summaries[i]\n",
    "    if generated_summary == reference_summaries[i]:\n",
    "        accuracy += 1.0\n",
    "accuracy = accuracy / len(reference_summaries)\n",
    "\n",
    "print(f\"Achieved accuracy {accuracy}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Accuracy zero?!?  We can see that the (very generic) metric of 0/1 accuracy is not useful for summarization.  Thinking about this more, small variations in wording may not matter much, and many different summaries may be equally valid.  So how can we evaluate summarization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE\n",
    "Now that we can generate summaries---and we know 0/1 accuracy is useless here---let's look at how we can compute a meaningful metric designed to evaluate summarization: ROUGE.\n",
    "\n",
    "Recall-Oriented Understudy for Gisting Evaluation (ROUGE) is a set of evaluation metrics designed for comparing summaries from Lin et al., 2004.  See [Wikipedia](https://en.wikipedia.org/wiki/ROUGE_&#40;metric&#41;) for more info.  Here, we use the Hugging Face Evaluator wrapper to call into the `rouge_score` package.  This package provides 4 scores:\n",
    "\n",
    "* `rouge1`: ROUGE computed over unigrams (single words or tokens)\n",
    "* `rouge2`: ROUGE computed over bigrams (pairs of consecutive words or tokens)\n",
    "* `rougeL`: ROUGE based on the longest common subsequence shared by the summaries being compared\n",
    "* `rougeLsum`: like `rougeL`, but at \"summary level,\" i.e., ignoring sentence breaks (newlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "rouge_score = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " You can call `rouge_score` evaluator directly, but we provide a convenience function below to handle the expected input format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rouge_score(generated: list, reference: list) -> dict:\n",
    "    \"\"\"\n",
    "    Compute ROUGE scores on a batch of articles.\n",
    "\n",
    "    This is a convenience function wrapping Hugging Face `rouge_score`,\n",
    "    which expects sentences to be separated by newlines.\n",
    "\n",
    "    :param generated: Summaries (list of strings) produced by the model\n",
    "    :param reference: Ground-truth summaries (list of strings) for comparison\n",
    "    \"\"\"\n",
    "    generated_with_newlines = [\"\\n\".join(sent_tokenize(s.strip())) for s in generated]\n",
    "    reference_with_newlines = [\"\\n\".join(sent_tokenize(s.strip())) for s in reference]\n",
    "    return rouge_score.compute(\n",
    "        predictions=generated_with_newlines,\n",
    "        references=reference_with_newlines,\n",
    "        use_stemmer=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE scores for our batch of articles\n",
    "compute_rouge_score(t5_small_summaries, reference_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Understanding ROUGE scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: What if our predictions match the references exactly?\n",
    "compute_rouge_score(reference_summaries, reference_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And what if we fail to predict anything?\n",
    "compute_rouge_score(\n",
    "    generated=[\"\" for _ in range(len(reference_summaries))],\n",
    "    reference=reference_summaries,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming predictions and references can help to ignore minor differences.\n",
    "\n",
    "We will use `rouge_score.compute()` directly for these hand-constructed examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_score.compute(\n",
    "    predictions=[\"Large language models beat world record\"],\n",
    "    references=[\"Large language models beating world records\"],\n",
    "    use_stemmer=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_score.compute(\n",
    "    predictions=[\"Large language models beat world record\"],\n",
    "    references=[\"Large language models beating world records\"],\n",
    "    use_stemmer=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's look at how the ROUGE score behaves in various situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we predict exactly 1 word correctly?\n",
    "rouge_score.compute(\n",
    "    predictions=[\"Large language models beat world record\"],\n",
    "    references=[\"Large\"],\n",
    "    use_stemmer=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ROUGE score is symmetric with respect to predictions and references.\n",
    "rouge_score.compute(\n",
    "    predictions=[\"Large\"],\n",
    "    references=[\"Large language models beat world record\"],\n",
    "    use_stemmer=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about 2 words?  Note how 'rouge1' and 'rouge2' compare with the case when we predict exactly 1 word correctly.\n",
    "rouge_score.compute(\n",
    "    predictions=[\"Large language\"],\n",
    "    references=[\"Large language models beat world record\"],\n",
    "    use_stemmer=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note how rouge1 differs from the rougeN (N>1) scores when we predict word subsequences correctly.\n",
    "rouge_score.compute(\n",
    "    predictions=[\"Models beat large language world record\"],\n",
    "    references=[\"Large language models beat world record\"],\n",
    "    use_stemmer=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAGIC  %md ## Compare small and large models\n",
    "# MAGIC\n",
    "# MAGIC  We've been working with the `t5-small` model so far.  Let's compare several models with different architectures in terms of their ROUGE scores and some example generated summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rouge_per_row(\n",
    "    generated_summaries: list, reference_summaries: list\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generates a dataframe to compare rogue score metrics.\n",
    "    \"\"\"\n",
    "    generated_with_newlines = [\n",
    "        \"\\n\".join(sent_tokenize(s.strip())) for s in generated_summaries\n",
    "    ]\n",
    "    reference_with_newlines = [\n",
    "        \"\\n\".join(sent_tokenize(s.strip())) for s in reference_summaries\n",
    "    ]\n",
    "    scores = rouge_score.compute(\n",
    "        predictions=generated_with_newlines,\n",
    "        references=reference_with_newlines,\n",
    "        use_stemmer=True,\n",
    "        use_aggregator=False,\n",
    "    )\n",
    "    scores[\"generated\"] = generated_summaries\n",
    "    scores[\"reference\"] = reference_summaries\n",
    "    return pd.DataFrame.from_dict(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5-small\n",
    "The [T5](https://huggingface.co/docs/transformers/model_doc/t5) [[paper]](https://arxiv.org/pdf/1910.10683.pdf) family of models are text-to-text transformers that have been trained on a multi-task mixture of unsupervised and supervised tasks. They are well suited for task such as summarization, translation, text classification, question answering, and more.\n",
    "\n",
    "The t5-small version of the T5 models has 60 million parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We computed t5_small_summaries above already.\n",
    "compute_rouge_score(t5_small_summaries, reference_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_small_results = compute_rouge_per_row(\n",
    "    generated_summaries=t5_small_summaries, reference_summaries=reference_summaries\n",
    ")\n",
    "display(t5_small_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5-base\n",
    "The [T5-base](https://huggingface.co/t5-base) model has 220 million parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_base_summaries = summarize_with_t5(\n",
    "    model_checkpoint=\"t5-base\", articles=sample[\"article\"]\n",
    ")\n",
    "compute_rouge_score(t5_base_summaries, reference_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_base_results = compute_rouge_per_row(\n",
    "    generated_summaries=t5_base_summaries, reference_summaries=reference_summaries\n",
    ")\n",
    "display(t5_base_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-2\n",
    "The [GPT-2](https://huggingface.co/gpt2) model is a generative text model that was trained in a self-supervised fashion. Its strengths are in using a 'completing the sentence' for a given prompt.  It has 124 million parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "\n",
    "def summarize_with_gpt2(\n",
    "    model_checkpoint: str, articles: list, batch_size: int = 8\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Convenience function for summarization with GPT2 to handle these complications:\n",
    "    - Append \"TL;DR\" to the end of the input to get GPT2 to generate a summary.\n",
    "    https://huggingface.co/course/chapter7/5?fw=pt\n",
    "    - Truncate input to handle long articles.\n",
    "    - GPT2 uses a max token length of 1024.  We use a shorter 512 limit here.\n",
    "\n",
    "    :param model_checkpoint: reference to checkpointed model\n",
    "    :param articles: list of strings\n",
    "    :return: generated summaries, with the input and \"TL;DR\" removed\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\n",
    "        model_checkpoint, padding_side=\"left\", cache_dir=DA.paths.datasets\n",
    "    )\n",
    "    tokenizer.add_special_tokens({\"pad_token\": tokenizer.eos_token})\n",
    "    model = GPT2LMHeadModel.from_pretrained(\n",
    "        model_checkpoint,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        cache_dir=DA.paths.datasets,\n",
    "    ).to(device)\n",
    "\n",
    "    def perform_inference(batch: list) -> list:\n",
    "        tmp_inputs = tokenizer(\n",
    "            batch, max_length=500, return_tensors=\"pt\", padding=True, truncation=True\n",
    "        )\n",
    "        tmp_inputs_decoded = tokenizer.batch_decode(\n",
    "            tmp_inputs.input_ids, skip_special_tokens=True\n",
    "        )\n",
    "        inputs = tokenizer(\n",
    "            [article + \" TL;DR:\" for article in tmp_inputs_decoded],\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "        )\n",
    "        summary_ids = model.generate(\n",
    "            inputs.input_ids.to(device),\n",
    "            attention_mask=inputs.attention_mask.to(device),\n",
    "            num_beams=2,\n",
    "            min_length=0,\n",
    "            max_length=512 + 32,\n",
    "        )\n",
    "        return tokenizer.batch_decode(summary_ids, skip_special_tokens=True)\n",
    "\n",
    "    decoded_summaries = []\n",
    "    for batch in batch_generator(articles, batch_size=batch_size):\n",
    "        decoded_summaries += perform_inference(batch)\n",
    "\n",
    "        # batch clean up\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # post-process decoded summaries\n",
    "    summaries = [\n",
    "        summary[summary.find(\"TL;DR:\") + len(\"TL;DR: \") :]\n",
    "        for summary in decoded_summaries\n",
    "    ]\n",
    "\n",
    "    # cleanup\n",
    "    del tokenizer\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_summaries = summarize_with_gpt2(\n",
    "    model_checkpoint=\"gpt2\", articles=sample[\"article\"]\n",
    ")\n",
    "compute_rouge_score(gpt2_summaries, reference_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_results = compute_rouge_per_row(\n",
    "    generated_summaries=gpt2_summaries, reference_summaries=reference_summaries\n",
    ")\n",
    "display(gpt2_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing all models\n",
    "We use a couple of helper functions to compare the above models, first by their evaluation metrics (quantitative) and second by their generated summaries (qualitative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(models_results: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    :param models_results: dict of \"model name\" string mapped to pd.DataFrame of results computed by `compute_rouge_per_row`\n",
    "    :return: pd.DataFrame with 1 row per model, with columns: model, rouge1, rouge2, rougeL, rougeLsum\n",
    "    where metrics are averages over input results for each model\n",
    "    \"\"\"\n",
    "    agg_results = []\n",
    "    for r in models_results:\n",
    "        model_results = models_results[r].drop(\n",
    "            labels=[\"generated\", \"reference\"], axis=1\n",
    "        )\n",
    "        agg_metrics = [r]\n",
    "        agg_metrics[1:] = model_results.mean(axis=0)\n",
    "        agg_results.append(agg_metrics)\n",
    "    return pd.DataFrame(\n",
    "        agg_results, columns=[\"model\", \"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    compare_models(\n",
    "        {\n",
    "            \"t5-small\": t5_small_results,\n",
    "            \"t5-base\": t5_base_results,\n",
    "            \"gpt2\": gpt2_results,\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models_summaries(models_summaries: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregates results from `models_summaries` and returns a dataframe.\n",
    "    \"\"\"\n",
    "    comparison_df = None\n",
    "    for model_name in models_summaries:\n",
    "        summaries_df = models_summaries[model_name]\n",
    "        if comparison_df is None:\n",
    "            comparison_df = summaries_df[[\"generated\"]].rename(\n",
    "                {\"generated\": model_name}, axis=1\n",
    "            )\n",
    "        else:\n",
    "            comparison_df = comparison_df.join(\n",
    "                summaries_df[[\"generated\"]].rename({\"generated\": model_name}, axis=1)\n",
    "            )\n",
    "    return comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the output table below, scroll to the right to see all models.\n",
    "display(\n",
    "    compare_models_summaries(\n",
    "        {\n",
    "            \"t5_small\": t5_small_results,\n",
    "            \"t5_base\": t5_base_results,\n",
    "            \"gpt2\": gpt2_results,\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-sandbox\n",
    "&copy; 2023 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
