{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-sandbox\n",
        "# MAGIC\n",
        "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
        "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MAGIC\n",
        "# Lab: Adding Our Own Data to a Multi-Stage Reasoning System\n",
        "# MAGIC\n",
        "### Working with external knowledge bases \n",
        "In this notebook we're going to augment the knowledge base of our LLM with additional data. We will split the notebook into two halves:\n",
        "- First, we will walk through how to load in a relatively small, local text file using a `DocumentLoader`, split it into chunks, and store it in a vector database using `ChromaDB`.\n",
        "- Second, you will get a chance to show what you've learned by building a larger system with the complete works of Shakespeare. \n",
        "----\n",
        "### ![Dolly](https://files.training.databricks.com/images/llm/dolly_small.png) Learning Objectives\n",
        "# MAGIC\n",
        "By the end of this notebook, you will be able to:\n",
        "1. Add external local data to your LLM's knowledge base via a vector database.\n",
        "2. Construct a Question-Answer(QA) LLMChain to \"talk to your data.\"\n",
        "3. Load external data sources from remote locations and store in a vector database.\n",
        "4. Leverage different retrieval methods to search over your data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MAGIC\n",
        "## Classroom Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MAGIC %run ../Includes/Classroom-Setup"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " Import libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MAGIC %pip install chromadb==0.3.21 tiktoken==0.3.3 sqlalchemy==2.0.15"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " Fill in your credentials."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO\n",
        "# For many of the services that we'll using in the notebook, we'll need a HuggingFace API key so this cell will ask for it:\n",
        "# HuggingFace Hub: https://huggingface.co/inference-api\n",
        "\n",
        "import os\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"<FILL IN>\""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \n",
        "## Building a Personalized Document Oracle\n",
        "# MAGIC\n",
        "In this notebook, we're going to build a special type of LLMChain that will enable us to ask questions of our data. We will be able to \"speak to our data\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \n",
        "### Step 1 - Loading Documents into our Vector Store\n",
        "For this system we'll leverage the [ChromaDB vector database](https://www.trychroma.com/) and load in some text we have on file. This file is of a hypothetical laptop being reviewed in both long form and with brief customer reviews. We'll use LangChain's `TextLoader` to load this data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "# We have some fake laptop reviews that we can load in\n",
        "laptop_reviews = TextLoader(\n",
        "    f\"{DA.paths.datasets}/reviews/fake_laptop_reviews.txt\", encoding=\"utf8\"\n",
        ")\n",
        "document = laptop_reviews.load()\n",
        "display(document)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \n",
        "### Step 2 - Chunking and Embeddings\n",
        "# MAGIC\n",
        "Now that we have the data in document format, we will split data into chunks using a `CharacterTextSplitter` and embed this data using Hugging Face's embedding LLM to embed this data for our vector store."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# First we split the data into manageable chunks to store as vectors. There isn't an exact way to do this, more chunks means more detailed context, but will increase the size of our vectorstore.\n",
        "text_splitter = CharacterTextSplitter(chunk_size=250, chunk_overlap=10)\n",
        "texts = text_splitter.split_documents(document)\n",
        "# Now we'll create embeddings for our document so we can store it in a vector store and feed the data into an LLM. We'll use the sentence-transformers model for out embeddings. https://www.sbert.net/docs/pretrained_models.html#sentence-embedding-models/\n",
        "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=model_name, cache_folder=DA.paths.datasets\n",
        ")  # Use a pre-cached model\n",
        "# Finally we make our Index using chromadb and the embeddings LLM\n",
        "chromadb_index = Chroma.from_documents(\n",
        "    texts, embeddings, persist_directory=DA.paths.working_dir\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \n",
        "### Step 3 - Creating our Document QA LLM Chain\n",
        "With our data now in vector form we need an LLM and a chain to take our queries and create tasks for our LLM to perform. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "# We want to make this a retriever, so we need to convert our index.  This will create a wrapper around the functionality of our vector database so we can search for similar documents/chunks in the vectorstore and retrieve the results:\n",
        "retriever = chromadb_index.as_retriever()\n",
        "\n",
        "# This chain will be used to do QA on the document. We will need\n",
        "# 1 - A LLM to do the language interpretation\n",
        "# 2 - A vector database that can perform document retrieval\n",
        "# 3 - Specification on how to deal with this data (more on this soon)\n",
        "\n",
        "hf_llm = HuggingFacePipeline.from_model_id(\n",
        "    model_id=\"google/flan-t5-large\",\n",
        "    task=\"text2text-generation\",\n",
        "    model_kwargs={\n",
        "        \"temperature\": 0,\n",
        "        \"max_length\": 128,\n",
        "        \"cache_dir\": DA.paths.datasets,\n",
        "    },\n",
        ")\n",
        "\n",
        "chain_type = \"stuff\"  # Options: stuff, map_reduce, refine, map_rerank\n",
        "laptop_qa = RetrievalQA.from_chain_type(\n",
        "    llm=hf_llm, chain_type=\"stuff\", retriever=retriever\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \n",
        "### Step 4 - Talking to Our Data\n",
        "Now we are ready to send prompts to our LLM and have it use our prompt, the access to our data, and read the information, process, and return with a response."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Let's ask the chain about the product we have.\n",
        "laptop_name = laptop_qa.run(\"What is the full name of the laptop?\")\n",
        "display(laptop_name)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Now we'll ask the chain about the product.\n",
        "laptop_features = laptop_qa.run(\"What are some of the laptop's features?\")\n",
        "display(laptop_features)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Finally let's ask the chain about the reviews.\n",
        "laptop_reviews = laptop_qa.run(\"What is the general sentiment of the reviews?\")\n",
        "display(laptop_reviews)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \n",
        "## Exercise: Working with larger documents\n",
        "This document was relatively small. So let's see if we can work with something bigger. To show how well we can scale the vector database, let's load in a larger document. For this we'll get data from the [Gutenberg Project](https://www.gutenberg.org/) where thousands of free-to-access texts. We'll use the complete works of William Shakespeare.\n",
        "# MAGIC\n",
        "Instead of a local text document, we'll download the complete works of Shakespeare using the `GutenbergLoader` that works with the Gutenberg project: https://www.gutenberg.org"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.document_loaders import GutenbergLoader\n",
        "\n",
        "loader = GutenbergLoader(\n",
        "    \"https://www.gutenberg.org/cache/epub/100/pg100.txt\"\n",
        ")  # Complete works of Shakespeare in a txt file\n",
        "\n",
        "all_shakespeare_text = loader.load()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 1\n",
        "# MAGIC\n",
        "Now it's your turn! Based on what we did previously, fill in the missing parts below to build your own QA LLMChain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO\n",
        "text_splitter = <FILL_IN> #hint try chunk sizes of 1024 and an overlap of 256 (this will take approx. 10mins with this model to build our vector database index)\n",
        "texts = <FILL_IN>\n",
        "\n",
        "model_name = <FILL_IN> #hint, try \"sentence-transformers/all-MiniLM-L6-v2\" as your model\n",
        "embeddings = <FILL_IN>\n",
        "docsearch = <FILL_IN>"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test your answer. DO NOT MODIFY THIS CELL.\n",
        "\n",
        "dbTestQuestion3_1(embeddings, docsearch)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \n",
        "### Question 2\n",
        "# MAGIC\n",
        "Let's see if we can do what we did with the laptop reviews. \n",
        "# MAGIC\n",
        "Think about what is likely to happen now. Will this command succeed? \n",
        "# MAGIC\n",
        "(***Hint: think about the maximum sequence length of a model***)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO\n",
        "# Let's start with the simplest method: \"Stuff\" which puts all of the data into the prompt and asks a question of it:\n",
        "qa = RetrievalQA.from_chain_type(<FILL_IN>)\n",
        "query = \"What happens in the play Hamlet?\"\n",
        "# Run the query\n",
        "query_results_hamlet = <FILL_IN>\n",
        "\n",
        "query_results_hamlet"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test your answer. DO NOT MODIFY THIS CELL.\n",
        "\n",
        "dbTestQuestion3_2(qa, query_results_hamlet)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \n",
        "### Question 3\n",
        "# MAGIC\n",
        "Now that we're working with larger documents, we should be mindful of the input sequence limitations that our LLM has. \n",
        "# MAGIC\n",
        "Chain Types for document loader:\n",
        "# MAGIC\n",
        "- [`stuff`](https://docs.langchain.com/docs/components/chains/index_related_chains#stuffing) - Stuffing is the simplest method, whereby you simply stuff all the related data into the prompt as context to pass to the language model.\n",
        "- [`map_reduce`](https://docs.langchain.com/docs/components/chains/index_related_chains#map-reduce) - This method involves running an initial prompt on each chunk of data (for summarization tasks, this could be a summary of that chunk; for question-answering tasks, it could be an answer based solely on that chunk).\n",
        "- [`refine`](https://docs.langchain.com/docs/components/chains/index_related_chains#refine) - This method involves running an initial prompt on the first chunk of data, generating some output. For the remaining documents, that output is passed in, along with the next document, asking the LLM to refine the output based on the new document.\n",
        "- [`map_rerank`](https://docs.langchain.com/docs/components/chains/index_related_chains#map-rerank) - This method involves running an initial prompt on each chunk of data, that not only tries to complete a task but also gives a score for how certain it is in its answer. The responses are then ranked according to this score, and the highest score is returned.\n",
        "  * NOTE: For this exercise, `map_rerank` will [error](https://github.com/hwchase17/langchain/issues/3970)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO\n",
        "qa = RetrievalQA.from_chain_type(llm=hf_llm, chain_type=<FILL_IN>, retriever=docsearch.as_retriever())\n",
        "query = \"Who is the main character in the Merchant of Venice?\"\n",
        "query_results_venice = <FILL_IN>\n",
        "\n",
        "query_results_venice"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test your answer. DO NOT MODIFY THIS CELL.\n",
        "\n",
        "dbTestQuestion3_3(qa, query_results_venice)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \n",
        "### Question 4\n",
        "# MAGIC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO\n",
        "# That's much better! Let's try another type\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(llm=hf_llm, chain_type=<FILL_IN>, retriever=docsearch.as_retriever())\n",
        "query = \"What happens to romeo and juliet?\"\n",
        "query_results_romeo = <FILL_IN>\n",
        "\n",
        "query_results_romeo"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test your answer. DO NOT MODIFY THIS CELL.\n",
        "\n",
        "dbTestQuestion3_4(qa, query_results_romeo)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ## Submit your Results (edX Verified Only)\n",
        "# MAGIC\n",
        "To get credit for this lab, click the submit button in the top right to report the results. If you run into any issues, click `Run` -> `Clear state and run all`, and make sure all tests have passed before re-submitting. If you accidentally deleted any tests, take a look at the notebook's version history to recover them or reload the notebooks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-sandbox\n",
        "&copy; 2023 Databricks, Inc. All rights reserved.<br/>\n",
        "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
        "<br/>\n",
        "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}