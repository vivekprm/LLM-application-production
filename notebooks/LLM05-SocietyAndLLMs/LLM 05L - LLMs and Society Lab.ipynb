{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-sandbox\n",
        "# MAGIC\n",
        "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
        "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MAGIC\n",
        "# LLMs and Society Lab\n",
        "# MAGIC\n",
        "### ![Dolly](https://files.training.databricks.com/images/llm/dolly_small.png) Learning Objectives\n",
        "1. Learn how to evaluate polarity towards certain demographic groups using `regard`\n",
        "    - We will first evaluate whether dancers are regarded differently from scientists\n",
        "    - You will then compute `regard` with other groups of your choice\n",
        "2. Test your language model by changing text using `sparknlp` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MAGIC %pip install nlptest==1.4.0"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MAGIC\n",
        "## Classroom Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MAGIC %run ../Includes/Classroom-Setup"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate Regard \n",
        "# MAGIC\n",
        "We will use the [BOLD dataset](https://huggingface.co/datasets/AlexaAI/bold), created by Alexa AI, that allows us to evaluate model fairness in English text generation. Specifically, we will use categories within this dataset to prompt the language model for text completion. Some example categories include:\n",
        "- gender \n",
        "- professions\n",
        "- religions\n",
        "# MAGIC\n",
        "Continuing from the demo, we will use the same `evaluate` library from Hugging Face, but leverage a separate module called `regard`. We evaluate model fairness from the angle of polarity or perception to see if one group is assigned a higher regard score than the other.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "bold = load_dataset(\n",
        "    \"AlexaAI/bold\", split=\"train\", cache_dir=DA.paths.datasets\n",
        ")  # Note: We specify cache_dir to use pre-cached data."
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this exercise, we will examine whether people in the `dance_occupations` are regarded any differently than those in `scientific_occupations`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from random import sample\n",
        "\n",
        "def generate_samples(category_name: str, n: int) -> list:\n",
        "    \"\"\"\n",
        "    Given a category, returns `n` samples\n",
        "    \"\"\"\n",
        "    bold_samples = sample([p for p in bold if p[\"category\"] == category_name], n)\n",
        "    return bold_samples\n",
        "\n",
        "science_bold = generate_samples(\"scientific_occupations\", 10)\n",
        "dance_bold = generate_samples(\"dance_occupations\", 10)\n",
        "\n",
        "print(\"Science example: \", science_bold[0])\n",
        "print(\"-\" * 60)\n",
        "print(\"Dance example: \", dance_bold[0])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MAGIC\n",
        "### Question 1\n",
        "Now, it's your turn to generate some samples. Run the following cell to get a complete list of categories covered by BOLD.\n",
        "# MAGIC\n",
        "If you need inspiration, try `American_actors` and `American_actresses`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "np.unique(bold[\"category\"])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO\n",
        "\n",
        "# Generate samples from BOLD dataset\n",
        "group1_bold = generate_samples(\"<FILL_IN>\", 10)\n",
        "group2_bold = generate_samples(\"<FILL_IN>\", 10)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test your answer. DO NOT MODIFY THIS CELL.\n",
        "\n",
        "dbTestQuestion5_1(group1_bold, group2_bold)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's get some prompts from each of the categories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "science_prompts = [p[\"prompts\"][0] for p in science_bold]\n",
        "dance_prompts = [p[\"prompts\"][0] for p in dance_bold]\n",
        "print(\"Science prompt example: \", science_prompts[0])\n",
        "print(\"Dance prompt example: \", dance_prompts[0])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MAGIC\n",
        "### Question 2\n",
        "It's your turn to get prompts from the samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO\n",
        "\n",
        "group1_prompts = [p[\"prompts\"][0] for p in <FILL_IN>]\n",
        "group2_prompts = [p[\"prompts\"][0] for p in <FILL_IN>]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test your answer. DO NOT MODIFY THIS CELL.\n",
        "\n",
        "dbTestQuestion5_2(group1_prompts, group2_prompts)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's put GPT-2 to test. Does our model complete the sentences with equal regard for both the scientist and the dancer? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from transformers import pipeline, AutoTokenizer\n",
        "\n",
        "text_generation = pipeline(\n",
        "    \"text-generation\", model=\"gpt2\", model_kwargs={\"cache_dir\": DA.paths.datasets}\n",
        ")  # Note: We specify cache_dir to use a pre-cached model.\n",
        "\n",
        "def complete_sentence(text_generation_pipeline: pipeline, prompts: list) -> list:\n",
        "    \"\"\"\n",
        "    Via a list of prompts a prompt list is appended to by the generated `text_generation_pipeline`.\n",
        "    \"\"\"\n",
        "    prompt_continuations = []\n",
        "    for prompt in prompts:\n",
        "        generation = text_generation_pipeline(\n",
        "            prompt, max_length=30, do_sample=False, pad_token_id=50256\n",
        "        )\n",
        "        continuation = generation[0][\"generated_text\"].replace(prompt, \"\")\n",
        "        prompt_continuations.append(continuation)\n",
        "    return prompt_continuations"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will now complete the sentences for the dancers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dance_continuation = complete_sentence(text_generation, dance_prompts)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, let's generate text for scientists."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "science_continuation = complete_sentence(text_generation, science_prompts)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 3\n",
        "Your turn to ask the model to complete sentences for each group! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO\n",
        "\n",
        "group1_continuation = complete_sentence(<FILL_IN>)\n",
        "group2_continuation = complete_sentence(<FILL_IN>)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test your answer. DO NOT MODIFY THIS CELL.\n",
        "\n",
        "dbTestQuestion5_3(group1_continuation, group2_continuation)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have the prompts and the completion examples by GPT-2, we can evaluate the differences in regard towards both groups. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import evaluate\n",
        "\n",
        "regard = evaluate.load(\"regard\", \"compare\", cache_dir=DA.paths.datasets)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Wow, based on the `positive` regard field, we see that people in scientific occupations are regarded much more positively than those in dance (refer to the `positive` field) ! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# this returns the regard scores of each string in the input list\n",
        "regard.compute(data=science_continuation, references=dance_continuation)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MAGIC\n",
        "### Question 4\n",
        "Now, compute regard score for your groups!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO\n",
        "\n",
        "regard.compute(data=<FILL_IN>, references=<FILL_IN>)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test your answer. DO NOT MODIFY THIS CELL.\n",
        "\n",
        "dbTestQuestion5_4(\n",
        "    regard.compute(data=group1_continuation, references=group2_continuation)\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bonus: NLP Test\n",
        "# MAGIC\n",
        "To switch gears a bit, we will now turn to looking at how we can test our NLP models and see how safe and effective they are using `nlptest`. The [library](https://nlptest.org/) is developed by SparkNLP and aims to provide user-friendly APIs to help evaluate models. This library was just released in April 2023. \n",
        "# MAGIC\n",
        "The test categories include:\n",
        "# MAGIC\n",
        "- Accuracy\n",
        "- Bias\n",
        "- Fairness\n",
        "- Representation\n",
        "- Robustness\n",
        "# MAGIC\n",
        "Currently, the library supports either `text-classification` or `ner` task.\n",
        "# MAGIC\n",
        "To start, we will use the `Harness` class to define what types of tests we would like to conduct on any given NLP model. You can read more about [Harness here](https://nlptest.org/docs/pages/docs/harness). The cell below provides a quick one-liner to show how you can evaluate the model, `dslim/bert-base-NER` from HuggingFace on a Named Entity Recognition (NER) task.\n",
        "# MAGIC\n",
        "You can choose to provide your own saved model or load existing models from `spacy` or `John Snow Labs` as well. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from nlptest import Harness\n",
        "\n",
        "# Create a Harness object\n",
        "h = Harness(task=\"ner\", model=\"dslim/bert-base-NER\", hub=\"huggingface\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We won't run the following cell since it could take up to 7 mins. This is a one-liner that runs all tests against the language model you supply. \n",
        "# MAGIC\n",
        "Notice that it consists of three steps: \n",
        "1. Generate test cases\n",
        "2. Run the test cases\n",
        "3. Generate a report of your test cases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# h.generate().run().report()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you do run `h.generate().run.report()` above, you can see that the report generates different test cases from different `test_type` and `category`. Specifically, it's unsurprising to see that the model fails the `lowercase` test for a NER use case. After all, if we lowercase all names, it would be hard to tell if the names are indeed referring to proper nouns, e.g. \"the los angeles time\" vs. \"the Los Angeles Times\".\n",
        "# MAGIC\n",
        "You can get a complete list of tests in their [documentation](https://nlptest.org/docs/pages/tests/test). For example, for `add_typo`, it checks whether the NLP model we use can handle input text with typos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ## Submit your Results (edX Verified Only)\n",
        "# MAGIC\n",
        "To get credit for this lab, click the submit button in the top right to report the results. If you run into any issues, click `Run` -> `Clear state and run all`, and make sure all tests have passed before re-submitting. If you accidentally deleted any tests, take a look at the notebook's version history to recover them or reload the notebooks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-sandbox\n",
        "&copy; 2023 Databricks, Inc. All rights reserved.<br/>\n",
        "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
        "<br/>\n",
        "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}