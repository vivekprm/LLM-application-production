{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd01710c-c8f3-4b6c-be10-f9dbf786bae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vmishra/miniconda3/envs/llm/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f68d20e-1cd2-4cb1-a3c9-d54df571f6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 800\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 200\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': '<s>[INST] Convierte el siguiente texto en tercera persona:\\n\"Soy un investigador con una sólida formación en Ingeniería Electrónica obtenida en la UTN. Tengo experiencia en aplicaciones ópticas, adquirida durante mi tiempo como becario en el Laboratorio de Holografía, y he participado en diversos proyectos de investigación y desarrollo.\" [/INST] Él es un investigador con una sólida formación en Ingeniería Electrónica obtenida en la UTN. Tiene experiencia en aplicaciones ópticas, adquirida durante su tiempo como becario en el Laboratorio de Holografía, y ha participado en diversos proyectos de investigación y desarrollo. </s>'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset\n",
    "data_name = \"mlabonne/guanaco-llama2-1k\"\n",
    "raw_datasets = load_dataset(data_name, cache_dir=\"../../datasets\", split=\"train\")\n",
    "raw_datasets = raw_datasets.train_test_split(test_size=0.2, shuffle=True)\n",
    "print(raw_datasets)\n",
    "raw_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f757c136-537a-4dd8-ad34-86412a1cac68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and tokenizer names\n",
    "base_model_name = \"gpt2\"\n",
    "refined_model = \"gpt2-enhanced\"\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "\n",
    "device_type = \"mps\"\n",
    "device = torch.device(device_type)\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, cache_dir=\"../../datasets\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"  # Fix for fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bec8626c-dd41-45e3-bb48-91bacdfc520e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b35244b562e747538a8d5a51d2cc552d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c326cd9324a49159d671c7a44574729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    \n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e3b94a0-2008-4300-8a35-5e6166483549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 800\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6641af80-c77c-4cc7-841f-7fbefbbf0e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# Quantization Config\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False\n",
    ")\n",
    "# Model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name\n",
    ")\n",
    "\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98ba8308-0822-4ec0-97e0-8c77f925e4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, PeftModel\n",
    "# LoRA Config\n",
    "peft_parameters = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=8,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12eac1e9-5a03-4640-8c91-56f678c36318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '<s>[INST] ¿En qué materiales se puede imprimir en 3D? [/INST] La impresión 3D utiliza una variedad de materiales para crear objetos tridimensionales. A continuación se presentan algunos de los materiales más comunes utilizados en la impresión 3D:\\n\\nPlásticos: el plástico es uno de los materiales más utilizados en la impresión 3D. Algunos de los plásticos más comunes incluyen ABS, PLA, PETG, Nylon, TPU, entre otros.\\n\\nResinas: la resina es un material líquido que se endurece cuando se expone a la luz ultravioleta. Este tipo de material se utiliza en impresoras 3D de resina y es común en la producción de joyas, moldes, y piezas detalladas.\\n\\nMetales: la impresión 3D de metales se ha vuelto más común en los últimos años y se utiliza en la producción de piezas aeroespaciales, automotrices, y médicas. Algunos de los metales comunes utilizados en la impresión 3D son aluminio, titanio, acero inoxidable y cobre.\\n\\nCerámica: la impresión 3D de cerámica se utiliza en la producción de objetos decorativos, esculturas, y piezas para la industria alimentaria. La cerámica se imprime en capas y se cuece en un horno para endurecer el material.\\n\\nMadera: la impresión 3D de madera se utiliza en la producción de objetos de arte, joyería, y piezas de decoración. La madera se mezcla con otros materiales para darle rigidez y durabilidad.\\n\\nComposites: los materiales compuestos son combinaciones de dos o más materiales diferentes. La impresión 3D de composites se utiliza en la producción de piezas automotrices, aeroespaciales y marinas.\\n\\nAlimentos: la impresión 3D de alimentos se utiliza en la producción de piezas alimenticias personalizadas. Los alimentos se imprimen en capas y se cocinan para endurecerlos. Los materiales alimentarios comunes incluyen chocolate, pasta, y queso. </s>',\n",
       " 'input_ids': [27,\n",
       "  82,\n",
       "  36937,\n",
       "  38604,\n",
       "  60,\n",
       "  1587,\n",
       "  123,\n",
       "  4834,\n",
       "  627,\n",
       "  2634,\n",
       "  2587,\n",
       "  274,\n",
       "  384,\n",
       "  279,\n",
       "  1739,\n",
       "  68,\n",
       "  848,\n",
       "  3036,\n",
       "  343,\n",
       "  551,\n",
       "  513,\n",
       "  35,\n",
       "  30,\n",
       "  46581,\n",
       "  38604,\n",
       "  60,\n",
       "  4689,\n",
       "  848,\n",
       "  411,\n",
       "  72,\n",
       "  18840,\n",
       "  513,\n",
       "  35,\n",
       "  7736,\n",
       "  23638,\n",
       "  555,\n",
       "  64,\n",
       "  15641,\n",
       "  324,\n",
       "  390,\n",
       "  2587,\n",
       "  274,\n",
       "  31215,\n",
       "  1126,\n",
       "  283,\n",
       "  26181,\n",
       "  316,\n",
       "  418,\n",
       "  491,\n",
       "  312,\n",
       "  16198,\n",
       "  274,\n",
       "  13,\n",
       "  317,\n",
       "  11143,\n",
       "  32009,\n",
       "  18840,\n",
       "  384,\n",
       "  1944,\n",
       "  272,\n",
       "  435,\n",
       "  7145,\n",
       "  418,\n",
       "  390,\n",
       "  22346,\n",
       "  2587,\n",
       "  274,\n",
       "  285,\n",
       "  40138,\n",
       "  401,\n",
       "  4015,\n",
       "  7736,\n",
       "  528,\n",
       "  22484,\n",
       "  551,\n",
       "  8591,\n",
       "  848,\n",
       "  411,\n",
       "  72,\n",
       "  18840,\n",
       "  513,\n",
       "  35,\n",
       "  25,\n",
       "  198,\n",
       "  198,\n",
       "  3646,\n",
       "  6557,\n",
       "  11268,\n",
       "  418,\n",
       "  25,\n",
       "  1288,\n",
       "  458,\n",
       "  6557,\n",
       "  301,\n",
       "  3713,\n",
       "  1658,\n",
       "  555,\n",
       "  78,\n",
       "  390,\n",
       "  22346,\n",
       "  2587,\n",
       "  274,\n",
       "  285,\n",
       "  40138,\n",
       "  7736,\n",
       "  528,\n",
       "  22484,\n",
       "  551,\n",
       "  8591,\n",
       "  848,\n",
       "  411,\n",
       "  72,\n",
       "  18840,\n",
       "  513,\n",
       "  35,\n",
       "  13,\n",
       "  978,\n",
       "  7145,\n",
       "  418,\n",
       "  390,\n",
       "  22346,\n",
       "  458,\n",
       "  6557,\n",
       "  11268,\n",
       "  418,\n",
       "  285,\n",
       "  40138,\n",
       "  401,\n",
       "  4015,\n",
       "  13358,\n",
       "  4669,\n",
       "  268,\n",
       "  29950,\n",
       "  11,\n",
       "  45838,\n",
       "  11,\n",
       "  32043,\n",
       "  38,\n",
       "  11,\n",
       "  399,\n",
       "  15158,\n",
       "  11,\n",
       "  309,\n",
       "  5105,\n",
       "  11,\n",
       "  920,\n",
       "  260,\n",
       "  30972,\n",
       "  4951,\n",
       "  13,\n",
       "  198,\n",
       "  198,\n",
       "  4965,\n",
       "  24252,\n",
       "  25,\n",
       "  8591,\n",
       "  581,\n",
       "  1437,\n",
       "  1658,\n",
       "  555,\n",
       "  2587,\n",
       "  300,\n",
       "  8836,\n",
       "  421,\n",
       "  17305,\n",
       "  8358,\n",
       "  384,\n",
       "  21178,\n",
       "  344,\n",
       "  18912,\n",
       "  25440,\n",
       "  384,\n",
       "  1033,\n",
       "  505,\n",
       "  257,\n",
       "  8591,\n",
       "  300,\n",
       "  10277,\n",
       "  49961,\n",
       "  64,\n",
       "  13,\n",
       "  412,\n",
       "  4169,\n",
       "  8171,\n",
       "  78,\n",
       "  390,\n",
       "  2587,\n",
       "  384,\n",
       "  7736,\n",
       "  23638,\n",
       "  551,\n",
       "  848,\n",
       "  411,\n",
       "  41043,\n",
       "  513,\n",
       "  35,\n",
       "  390,\n",
       "  581,\n",
       "  1437,\n",
       "  331,\n",
       "  1658,\n",
       "  401,\n",
       "  21356,\n",
       "  77,\n",
       "  551,\n",
       "  8591,\n",
       "  990,\n",
       "  35764,\n",
       "  18840,\n",
       "  390,\n",
       "  8716,\n",
       "  292,\n",
       "  11,\n",
       "  15936,\n",
       "  274,\n",
       "  11,\n",
       "  331,\n",
       "  2508,\n",
       "  89,\n",
       "  292,\n",
       "  1062,\n",
       "  439,\n",
       "  38768,\n",
       "  13,\n",
       "  198,\n",
       "  198,\n",
       "  9171,\n",
       "  2040,\n",
       "  25,\n",
       "  8591,\n",
       "  848,\n",
       "  411,\n",
       "  72,\n",
       "  18840,\n",
       "  513,\n",
       "  35,\n",
       "  390,\n",
       "  1138,\n",
       "  2040,\n",
       "  384,\n",
       "  387,\n",
       "  410,\n",
       "  2731,\n",
       "  1462,\n",
       "  285,\n",
       "  40138,\n",
       "  401,\n",
       "  21356,\n",
       "  77,\n",
       "  551,\n",
       "  22346,\n",
       "  6184,\n",
       "  118,\n",
       "  2528,\n",
       "  320,\n",
       "  418,\n",
       "  257,\n",
       "  12654,\n",
       "  418,\n",
       "  331,\n",
       "  384,\n",
       "  7736,\n",
       "  23638,\n",
       "  551,\n",
       "  8591,\n",
       "  990,\n",
       "  35764,\n",
       "  18840,\n",
       "  390,\n",
       "  2508,\n",
       "  89,\n",
       "  292,\n",
       "  9551,\n",
       "  3028,\n",
       "  79,\n",
       "  18150,\n",
       "  274,\n",
       "  11,\n",
       "  3557,\n",
       "  313,\n",
       "  45977,\n",
       "  11,\n",
       "  331,\n",
       "  285,\n",
       "  2634,\n",
       "  67,\n",
       "  44645,\n",
       "  13,\n",
       "  978,\n",
       "  7145,\n",
       "  418,\n",
       "  390,\n",
       "  22346,\n",
       "  1138,\n",
       "  2040,\n",
       "  401,\n",
       "  4015,\n",
       "  7736,\n",
       "  528,\n",
       "  22484,\n",
       "  551,\n",
       "  8591,\n",
       "  848,\n",
       "  411,\n",
       "  72,\n",
       "  18840,\n",
       "  513,\n",
       "  35,\n",
       "  3367,\n",
       "  435,\n",
       "  7230,\n",
       "  952,\n",
       "  11,\n",
       "  48047,\n",
       "  952,\n",
       "  11,\n",
       "  936,\n",
       "  3529,\n",
       "  287,\n",
       "  1140,\n",
       "  23321,\n",
       "  331,\n",
       "  22843,\n",
       "  260,\n",
       "  13,\n",
       "  198,\n",
       "  198,\n",
       "  34,\n",
       "  263,\n",
       "  6557,\n",
       "  76,\n",
       "  3970,\n",
       "  25,\n",
       "  8591,\n",
       "  848,\n",
       "  411,\n",
       "  72,\n",
       "  18840,\n",
       "  513,\n",
       "  35,\n",
       "  390,\n",
       "  26074,\n",
       "  6557,\n",
       "  76,\n",
       "  3970,\n",
       "  384,\n",
       "  7736,\n",
       "  23638,\n",
       "  551,\n",
       "  8591,\n",
       "  990,\n",
       "  35764,\n",
       "  18840,\n",
       "  390,\n",
       "  26181,\n",
       "  316,\n",
       "  418,\n",
       "  11705,\n",
       "  265,\n",
       "  452,\n",
       "  418,\n",
       "  11,\n",
       "  3671,\n",
       "  586,\n",
       "  17786,\n",
       "  11,\n",
       "  331,\n",
       "  2508,\n",
       "  89,\n",
       "  292,\n",
       "  31215,\n",
       "  8591,\n",
       "  2226,\n",
       "  7496,\n",
       "  435,\n",
       "  3681,\n",
       "  10312,\n",
       "  13,\n",
       "  4689,\n",
       "  26074,\n",
       "  6557,\n",
       "  76,\n",
       "  3970,\n",
       "  384,\n",
       "  848,\n",
       "  81,\n",
       "  524,\n",
       "  551,\n",
       "  1451,\n",
       "  292,\n",
       "  331,\n",
       "  384,\n",
       "  28381,\n",
       "  344,\n",
       "  551,\n",
       "  555,\n",
       "  12718,\n",
       "  78,\n",
       "  31215,\n",
       "  21178,\n",
       "  2189,\n",
       "  1288,\n",
       "  2587,\n",
       "  13,\n",
       "  198,\n",
       "  198,\n",
       "  44,\n",
       "  5067,\n",
       "  64,\n",
       "  25,\n",
       "  8591,\n",
       "  848,\n",
       "  411,\n",
       "  72,\n",
       "  18840,\n",
       "  513,\n",
       "  35,\n",
       "  390,\n",
       "  285,\n",
       "  5067,\n",
       "  64,\n",
       "  384,\n",
       "  7736,\n",
       "  23638,\n",
       "  551,\n",
       "  8591,\n",
       "  990,\n",
       "  35764,\n",
       "  18840,\n",
       "  390,\n",
       "  26181,\n",
       "  316,\n",
       "  418,\n",
       "  390,\n",
       "  46252,\n",
       "  11,\n",
       "  8716,\n",
       "  263,\n",
       "  29690,\n",
       "  11,\n",
       "  331,\n",
       "  2508,\n",
       "  89,\n",
       "  292,\n",
       "  390,\n",
       "  11705,\n",
       "  32009,\n",
       "  18840,\n",
       "  13,\n",
       "  4689,\n",
       "  285,\n",
       "  5067,\n",
       "  64,\n",
       "  384,\n",
       "  502,\n",
       "  89,\n",
       "  565,\n",
       "  64,\n",
       "  369,\n",
       "  30972,\n",
       "  4951,\n",
       "  2587,\n",
       "  274,\n",
       "  31215,\n",
       "  288,\n",
       "  283,\n",
       "  293,\n",
       "  7805,\n",
       "  485,\n",
       "  89,\n",
       "  331,\n",
       "  22365,\n",
       "  14991,\n",
       "  32482,\n",
       "  13,\n",
       "  198,\n",
       "  198,\n",
       "  5377,\n",
       "  1930,\n",
       "  2737,\n",
       "  25,\n",
       "  22346,\n",
       "  2587,\n",
       "  274,\n",
       "  552,\n",
       "  84,\n",
       "  395,\n",
       "  418,\n",
       "  3367,\n",
       "  1974,\n",
       "  259,\n",
       "  49443,\n",
       "  274,\n",
       "  390,\n",
       "  23430,\n",
       "  267,\n",
       "  285,\n",
       "  40138,\n",
       "  2587,\n",
       "  274,\n",
       "  288,\n",
       "  361,\n",
       "  9100,\n",
       "  274,\n",
       "  13,\n",
       "  4689,\n",
       "  848,\n",
       "  411,\n",
       "  72,\n",
       "  18840,\n",
       "  513,\n",
       "  35,\n",
       "  390,\n",
       "  18882,\n",
       "  2737,\n",
       "  384,\n",
       "  7736,\n",
       "  23638,\n",
       "  551,\n",
       "  8591,\n",
       "  990,\n",
       "  35764,\n",
       "  18840,\n",
       "  390,\n",
       "  2508,\n",
       "  89,\n",
       "  292,\n",
       "  3557,\n",
       "  313,\n",
       "  45977,\n",
       "  11,\n",
       "  9551,\n",
       "  3028,\n",
       "  79,\n",
       "  18150,\n",
       "  274,\n",
       "  331,\n",
       "  1667,\n",
       "  24252,\n",
       "  13,\n",
       "  198,\n",
       "  198,\n",
       "  2348,\n",
       "  3681,\n",
       "  418,\n",
       "  25,\n",
       "  8591,\n",
       "  848,\n",
       "  411,\n",
       "  72,\n",
       "  18840,\n",
       "  513,\n",
       "  35,\n",
       "  390,\n",
       "  435,\n",
       "  3681,\n",
       "  418,\n",
       "  384,\n",
       "  7736,\n",
       "  23638,\n",
       "  551,\n",
       "  8591,\n",
       "  990,\n",
       "  35764,\n",
       "  18840,\n",
       "  390,\n",
       "  2508,\n",
       "  89,\n",
       "  292,\n",
       "  435,\n",
       "  3681,\n",
       "  291,\n",
       "  4448,\n",
       "  2614,\n",
       "  528,\n",
       "  38768,\n",
       "  13,\n",
       "  5401,\n",
       "  435,\n",
       "  3681,\n",
       "  418,\n",
       "  384,\n",
       "  848,\n",
       "  3036,\n",
       "  268,\n",
       "  551,\n",
       "  1451,\n",
       "  292,\n",
       "  331,\n",
       "  384,\n",
       "  8954,\n",
       "  259,\n",
       "  272,\n",
       "  31215,\n",
       "  21178,\n",
       "  2189,\n",
       "  33280,\n",
       "  13,\n",
       "  5401,\n",
       "  2587,\n",
       "  274,\n",
       "  435,\n",
       "  3681,\n",
       "  13010,\n",
       "  401,\n",
       "  4015,\n",
       "  13358,\n",
       "  4669,\n",
       "  268,\n",
       "  11311,\n",
       "  11,\n",
       "  26296,\n",
       "  11,\n",
       "  331,\n",
       "  627,\n",
       "  274,\n",
       "  78,\n",
       "  13,\n",
       "  7359,\n",
       "  82,\n",
       "  29],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72e30f0d-6b48-4212-9a78-59dba1c26436",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'./peft-qa-training-{str(int(time.time()))}'\n",
    "\n",
    "# Training Params\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=1e-3, # Higher learning rate than full fine-tuning.\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=1,\n",
    "    max_steps=1    \n",
    ")\n",
    "    \n",
    "fine_tuning = Trainer(\n",
    "    model=base_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9700206-6816-40a8-b42b-9f14ade6ac91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The model did not return a loss from the inputs, only the following keys: logits. For reference, the inputs it received are input_ids,attention_mask.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mfine_tuning\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Save Model\u001b[39;00m\n\u001b[1;32m      4\u001b[0m fine_tuning\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave_pretrained(refined_model)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/trainer.py:1556\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1554\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1555\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1561\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/accelerate/utils/memory.py:136\u001b[0m, in \u001b[0;36mfind_executable_batch_size.<locals>.decorator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo executable batch size found, reached zero.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_reduce_batch_size(e):\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/trainer.py:1838\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1835\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1837\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1838\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1840\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1841\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1843\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1844\u001b[0m ):\n\u001b[1;32m   1845\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1846\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/trainer.py:2693\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2690\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2692\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2693\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2695\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2696\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/trainer.py:2735\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2733\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2734\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m outputs:\n\u001b[0;32m-> 2735\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2736\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model did not return a loss from the inputs, only the following keys: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2737\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(outputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. For reference, the inputs it received are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(inputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2738\u001b[0m         )\n\u001b[1;32m   2739\u001b[0m     \u001b[38;5;66;03m# We don't use .loss here since the model may return tuples instead of ModelOutput.\u001b[39;00m\n\u001b[1;32m   2740\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: The model did not return a loss from the inputs, only the following keys: logits. For reference, the inputs it received are input_ids,attention_mask."
     ]
    }
   ],
   "source": [
    "# Training\n",
    "fine_tuning.train()\n",
    "# Save Model\n",
    "fine_tuning.model.save_pretrained(refined_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cc5205-82d6-49f7-8865-cae2326b8c32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
